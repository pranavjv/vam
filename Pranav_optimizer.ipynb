{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Define VAM Optimizer\n",
    "# -----------------------------\n",
    "class VAM(Optimizer):\n",
    "    \"\"\"\n",
    "    Velocity-Adaptive Momentum (VAM) optimizer.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr, momentum=0.9, m=1.0, beta=0.1, eps=1e-8, weight_decay=0):\n",
    "        if lr <= 0.0:\n",
    "            raise ValueError(f\"Invalid lr: {lr}\")\n",
    "        if not 0.0 <= momentum < 1.0:\n",
    "            raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "        if m <= 0.0:\n",
    "            raise ValueError(f\"Invalid mass parameter m: {m}\")\n",
    "        if beta < 0.0:\n",
    "            raise ValueError(f\"Invalid beta: {beta}\")\n",
    "        defaults = dict(lr=lr, momentum=momentum, m=m, beta=beta,\n",
    "                        eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = closure() if closure is not None else None\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            mu = group['momentum']\n",
    "            m_base = group['m']\n",
    "            beta = group['beta']\n",
    "            eps = group['eps']\n",
    "            wd = group['weight_decay']\n",
    "\n",
    "            # 1) compute total squared norm of momentum buffers\n",
    "            total_sq_norm = 0.0\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                buf = state.get('momentum_buffer', None)\n",
    "                if buf is not None:\n",
    "                    total_sq_norm += float(buf.pow(2).sum())\n",
    "\n",
    "            # 2) adaptive learning rate\n",
    "            adaptive_lr = lr / (m_base + beta * total_sq_norm + eps)\n",
    "\n",
    "            # 3) per-parameter update\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad\n",
    "                if wd != 0:\n",
    "                    d_p = d_p.add(p, alpha=wd)\n",
    "\n",
    "                state = self.state[p]\n",
    "                buf = state.get('momentum_buffer', None)\n",
    "                if buf is None:\n",
    "                    buf = torch.zeros_like(p)\n",
    "                    state['momentum_buffer'] = buf\n",
    "\n",
    "                buf.mul_(mu).add_(d_p, alpha=-adaptive_lr)\n",
    "                p.add_(buf)\n",
    "\n",
    "        return loss\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Simple CNN Model\n",
    "# -----------------------------\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool  = nn.MaxPool2d(2, 2)\n",
    "        self.fc1   = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2   = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 32x32 -> 16x16\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 16x16 -> 8x8\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Data Loaders\n",
    "# -----------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914,0.4822,0.4465),(0.247,0.243,0.261))\n",
    "])\n",
    "\n",
    "train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_set  = datasets.CIFAR10(root='./data', train=False, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.4914,0.4822,0.4465),(0.247,0.243,0.261))\n",
    "                             ]))\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=4)\n",
    "test_loader  = DataLoader(test_set,  batch_size=256, shuffle=False, num_workers=4)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Train & Test Functions\n",
    "# -----------------------------\n",
    "def train_one_epoch(model, device, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = outputs.max(1)\n",
    "        correct += preds.eq(targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "def evaluate(model, device, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Benchmark Loop\n",
    "# -----------------------------\n",
    "def benchmark(optimizer_name, optimizer_cls, model_fn, optim_kwargs, device):\n",
    "    print(f\"\\n=== Benchmarking {optimizer_name} ===\")\n",
    "    model = model_fn().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optimizer_cls(model.parameters(), **optim_kwargs)\n",
    "\n",
    "    stats = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'test_loss':  [], 'test_acc':  []\n",
    "    }\n",
    "\n",
    "    start = time.time()\n",
    "    for epoch in range(1, 11):\n",
    "        tl, ta = train_one_epoch(model, device, train_loader, optimizer, criterion)\n",
    "        vl, va = evaluate(model, device, test_loader,  criterion)\n",
    "        stats['train_loss'].append(tl)\n",
    "        stats['train_acc'].append(ta)\n",
    "        stats['test_loss'].append(vl)\n",
    "        stats['test_acc'].append(va)\n",
    "        print(f\"Epoch {epoch:2d} | \"\n",
    "              f\"Train: loss={tl:.4f}, acc={ta:.4f} | \"\n",
    "              f\"Test: loss={vl:.4f}, acc={va:.4f}\")\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"{optimizer_name} finished in {elapsed/60:.2f} min\")\n",
    "    return stats\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "    benchmarks = {}\n",
    "\n",
    "    # 1) SGD with Momentum\n",
    "    benchmarks['SGD-Momentum'] = benchmark(\n",
    "        \"SGD with Momentum\",\n",
    "        optim.SGD,\n",
    "        SimpleCNN,\n",
    "        dict(lr=0.1, momentum=0.9, weight_decay=5e-4),\n",
    "        device\n",
    "    )\n",
    "\n",
    "    # 2) SGD with Nesterov\n",
    "    benchmarks['SGD-Nesterov'] = benchmark(\n",
    "        \"SGD with Nesterov\",\n",
    "        optim.SGD,\n",
    "        SimpleCNN,\n",
    "        dict(lr=0.1, momentum=0.9, nesterov=True, weight_decay=5e-4),\n",
    "        device\n",
    "    )\n",
    "\n",
    "    # 3) VAM\n",
    "    benchmarks['VAM'] = benchmark(\n",
    "        \"Velocity-Adaptive Momentum\",\n",
    "        VAM,\n",
    "        SimpleCNN,\n",
    "        dict(lr=0.1, momentum=0.9, m=1.0, beta=1.5, eps=1e-8, weight_decay=5e-4),\n",
    "        device\n",
    "    )\n",
    "\n",
    "    # Optionally: save benchmarks for later plotting\n",
    "    torch.save(benchmarks, \"benchmark_results.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
